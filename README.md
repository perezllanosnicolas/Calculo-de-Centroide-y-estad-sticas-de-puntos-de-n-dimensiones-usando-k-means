# ğŸš€ High-Performance Distributed K-Means (C++)

![C++](https://img.shields.io/badge/C++-14%2B-blue.svg)
![OpenMPI](https://img.shields.io/badge/OpenMPI-Distributed_Memory-orange.svg)
![OpenMP](https://img.shields.io/badge/OpenMP-Shared_Memory-green.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)

Este proyecto es una implementaciÃ³n de **Altas Prestaciones (HPC)** del algoritmo de clustering K-Means. DiseÃ±ado para procesar conjuntos de datos masivos aprovechando arquitecturas de **memoria distribuida (OpenMPI)** y **memoria compartida (OpenMP)**.

## ğŸ“Š Rendimiento y AceleraciÃ³n (Speedup)

El objetivo principal de este proyecto es exprimir al mÃ¡ximo el hardware subyacente. Partiendo de una implementaciÃ³n secuencial base, se han aplicado optimizaciones arquitectÃ³nicas severas:

* **Volumen de Datos:** Procesamiento de **8.000.000 de puntos** en **32 dimensiones** (Aprox. 1 GB de RAM en memoria continua).
* **Tiempo Secuencial Base:** ~81.68 segundos.
* **Tiempo HÃ­brido Optimizado:** **~2.74 segundos** (Media de 10 ejecuciones).
* **AceleraciÃ³n Total (Speedup):** **~30x**.

### TÃ©cnicas de OptimizaciÃ³n Aplicadas:
1. **Consciencia de CachÃ© L1/L2:** Uso de vectores contiguos en memoria 1D (`std::vector<float>`) para evitar el *cache-miss* de los punteros dobles y maximizar el pre-fetching.
2. **MetaprogramaciÃ³n (Templates):** Desenrollado de bucles en tiempo de compilaciÃ³n (*Loop Unrolling*) para dimensiones fijas, permitiendo auto-vectorizaciÃ³n SIMD profunda (`-O3 -march=native -ffast-math`).
3. **Paralelismo Bulk Synchronous (MPI):** ReducciÃ³n de la latencia de red agrupando los puntos en "buzones" locales y utilizando comunicaciÃ³n colectiva (`MPI_Alltoallv`, `MPI_Allreduce`) una sola vez por iteraciÃ³n.
4. **Multihilo Seguro (OpenMP):** Uso de buzones privados por hilo (Thread-local storage) y bloques `#pragma omp critical` para evitar *Race Conditions* manteniendo la CPU al 100% de uso.

## ğŸ¥ SimulaciÃ³n de la Arquitectura Distribuida

![AnimaciÃ³n K-Means Distribuido](docs/mpi_kmeans_architecture.gif)

> **Nota visual:** Esta simulaciÃ³n en 2D muestra el comportamiento real de la red. Los **4 colores** representan las memorias RAM fÃ­sicas de los **4 Nodos MPI** distintos. A medida que los centroides se mueven, los puntos viajan por la red cambiando de dueÃ±o (funciÃ³n `MPI_Alltoallv`) hasta alcanzar el equilibrio matemÃ¡tico.

## ğŸ“ Estructura del Proyecto

```text
ğŸ“¦ HPC-Distributed-KMeans
 â”£ ğŸ“‚ src/          # CÃ³digo fuente principal (K-Means distribuido, MPI, EstadÃ­sticas)
 â”£ ğŸ“‚ include/      # Archivos de cabecera (.h)
 â”£ ğŸ“‚ scripts/      # Utilidades en Python y scripts generadores de datos
 â”£ ğŸ“‚ docs/         # Recursos grÃ¡ficos y diagramas
 â”£ ğŸ“‚ data/         # Carpeta ignorada en Git para datasets binarios
 â”£ ğŸ“œ Makefile      # AutomatizaciÃ³n de la compilaciÃ³n
 â”£ ğŸ“œ Dockerfile    # Contenedor para ejecuciÃ³n aislada y reproducible
 â”— ğŸ“œ README.md

```

## âš™ï¸ Reproducibilidad y EjecuciÃ³n

Para garantizar que el cÃ³digo puede ser evaluado en cualquier sistema operativo sin lidiar con dependencias complejas de C++ y OpenMPI, el proyecto estÃ¡ completamente dockerizado.

### OpciÃ³n A: EjecuciÃ³n mediante Docker (Recomendada)
Requiere tener [Docker](https://www.docker.com/) instalado.

1. **Construir la imagen:**
   ```bash
   docker build -t hpc-kmeans .
   ```
2. **Ejecutar el clÃºster virtual (se autodestruye al terminar):**
   ```bash
   docker run --rm hpc-kmeans
   ```
   *Al ejecutar este comando, el contenedor generarÃ¡ automÃ¡ticamente el dataset masivo y lanzarÃ¡ la simulaciÃ³n con 4 nodos MPI, mostrando las mÃ©tricas y los cÃ¡lculos estadÃ­sticos en la terminal.*

### OpciÃ³n B: CompilaciÃ³n Nativa (Linux)
Si deseas ejecutarlo en tu propia mÃ¡quina de forma nativa, requieres `g++` y `openmpi-bin`.

```bash
# 1. Compilar todo el ecosistema
make

# 2. Generar el dataset masivo en la carpeta data/
./GeneradorPuntosNDimensiones

# 3. Lanzar OpenMPI con 4 procesos paralelos aislados
mpirun --mca btl_vader_single_copy_mechanism none -np 4 ./main_mpi
```

## ğŸ“ˆ CÃ¡lculo EstadÃ­stico Distribuido (Map-Reduce)
AdemÃ¡s del algoritmo K-Means, el sistema implementa un motor estadÃ­stico. Realiza dos pasadas Ã³ptimas por los datos segmentados en red para calcular **MÃ­nimo, MÃ¡ximo, Media y Varianza** de todas las dimensiones utilizando patrones de Map-Reduce (`MPI_Allreduce`) combinados con paralelismo hÃ­brido OpenMP.