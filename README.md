# High-Performance Distributed K-Means (C++)

![C++](https://img.shields.io/badge/C++-14%2B-blue.svg)
![OpenMPI](https://img.shields.io/badge/OpenMPI-Distributed_Memory-orange.svg)
![OpenMP](https://img.shields.io/badge/OpenMP-Shared_Memory-green.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)

Este proyecto es una implementaci√≥n de **Altas Prestaciones (HPC)** del algoritmo de clustering K-Means. Dise√±ado para procesar conjuntos de datos masivos aprovechando arquitecturas de **memoria distribuida (OpenMPI)** y **memoria compartida (OpenMP)**.

## Rendimiento y Aceleraci√≥n (Speedup)

El objetivo principal de este proyecto es exprimir al m√°ximo el hardware subyacente. Partiendo de una implementaci√≥n secuencial base, se han aplicado optimizaciones arquitect√≥nicas severas:

* **Volumen de Datos:** Procesamiento de **8.000.000 de puntos** en **32 dimensiones** (Aprox. 1 GB de RAM en memoria continua).
* **Tiempo Secuencial Base:** ~81.68 segundos.
* **Tiempo H√≠brido Optimizado:** **~2.74 segundos** (Media de 10 ejecuciones).
* **Aceleraci√≥n Total (Speedup):** **~30x**.

### T√©cnicas de Optimizaci√≥n Aplicadas:
1. **Consciencia de Cach√© L1/L2:** Uso de vectores contiguos en memoria 1D (`std::vector<float>`) para evitar el *cache-miss* de los punteros dobles y maximizar el pre-fetching.
2. **Metaprogramaci√≥n (Templates):** Desenrollado de bucles en tiempo de compilaci√≥n (*Loop Unrolling*) para dimensiones fijas, permitiendo auto-vectorizaci√≥n SIMD profunda (`-O3 -march=native -ffast-math`).
3. **Paralelismo Bulk Synchronous (MPI):** Reducci√≥n de la latencia de red agrupando los puntos en "buzones" locales y utilizando comunicaci√≥n colectiva (`MPI_Alltoallv`, `MPI_Allreduce`) una sola vez por iteraci√≥n.
4. **Multihilo Seguro (OpenMP):** Uso de buzones privados por hilo (Thread-local storage) y bloques `#pragma omp critical` para evitar *Race Conditions* manteniendo la CPU al 100% de uso.

## Simulaci√≥n de la Arquitectura Distribuida

![Animaci√≥n K-Means Distribuido](docs/mpi_kmeans_architecture.gif)

> **Nota visual:** Esta simulaci√≥n en 2D muestra el comportamiento real de la red. Los **4 colores** representan las memorias RAM f√≠sicas de los **4 Nodos MPI** distintos. A medida que los centroides se mueven, los puntos viajan por la red cambiando de due√±o (funci√≥n `MPI_Alltoallv`) hasta alcanzar el equilibrio matem√°tico.

## Estructura del Proyecto

```text
üì¶ HPC-Distributed-KMeans
 ‚î£ üìÇ src/          # C√≥digo fuente principal (K-Means distribuido, MPI, Estad√≠sticas)
 ‚î£ üìÇ include/      # Archivos de cabecera (.h)
 ‚î£ üìÇ scripts/      # Utilidades en Python y scripts generadores de datos
 ‚î£ üìÇ docs/         # Recursos gr√°ficos y diagramas
 ‚î£ üìÇ data/         # Carpeta ignorada en Git para datasets binarios
 ‚î£ üìú Makefile      # Automatizaci√≥n de la compilaci√≥n
 ‚î£ üìú Dockerfile    # Contenedor para ejecuci√≥n aislada y reproducible
 ‚îó üìú README.md

```
## Topolog√≠a de Ejecuci√≥n H√≠brida

![Arquitectura H√≠brida MPI+OpenMP](docs/hybrid_architecture.gif)

> **Flujo de Trabajo (BSP Model):** > 1. **(Azul) Distribuci√≥n:** El dataset se reparte de forma equitativa entre los procesos aislados de OpenMPI (`MPI_Scatter`).
> 2. **(Verde) Procesamiento:** Cada nodo despierta a sus hilos f√≠sicos mediante OpenMP (`#pragma omp parallel`) para calcular distancias simult√°neamente con memoria compartida local.
> 3. **(Rojo) Fusi√≥n:** Sincronizaci√≥n global y recolecci√≥n de los datos optimizados (`MPI_Allreduce`).


## Reproducibilidad y Ejecuci√≥n

Para garantizar que el c√≥digo puede ser evaluado en cualquier sistema operativo sin lidiar con dependencias complejas de C++ y OpenMPI, el proyecto est√° completamente dockerizado.

### Opci√≥n A: Ejecuci√≥n mediante Docker (Recomendada)
Requiere tener [Docker](https://www.docker.com/) instalado.

1. **Construir la imagen:**
   ```bash
   docker build -t hpc-kmeans .
   ```
2. **Ejecutar el cl√∫ster virtual (se autodestruye al terminar):**
   ```bash
   docker run --rm hpc-kmeans
   ```
   *Al ejecutar este comando, el contenedor generar√° autom√°ticamente el dataset masivo y lanzar√° la simulaci√≥n con 4 nodos MPI, mostrando las m√©tricas y los c√°lculos estad√≠sticos en la terminal.*

### Opci√≥n B: Compilaci√≥n Nativa (Linux)
Si deseas ejecutarlo en tu propia m√°quina de forma nativa, requieres `g++` y `openmpi-bin`.

```bash
# 1. Compilar todo el ecosistema
make

# 2. Generar el dataset masivo en la carpeta data/
./GeneradorPuntosNDimensiones

# 3. Lanzar OpenMPI con 4 procesos paralelos aislados
mpirun --mca btl_vader_single_copy_mechanism none -np 4 ./main_mpi
```

## C√°lculo Estad√≠stico Distribuido (Map-Reduce)
Adem√°s del algoritmo K-Means, el sistema implementa un motor estad√≠stico. Realiza dos pasadas √≥ptimas por los datos segmentados en red para calcular **M√≠nimo, M√°ximo, Media y Varianza** de todas las dimensiones utilizando patrones de Map-Reduce (`MPI_Allreduce`) combinados con paralelismo h√≠brido OpenMP.