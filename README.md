# ğŸš€ High-Performance Distributed K-Means (C++)

![C++](https://img.shields.io/badge/C++-14%2B-blue.svg)
![OpenMPI](https://img.shields.io/badge/OpenMPI-Distributed_Memory-orange.svg)
![OpenMP](https://img.shields.io/badge/OpenMP-Shared_Memory-green.svg)

Este proyecto es una implementaciÃ³n de **Altas Prestaciones (HPC)** del algoritmo de clustering K-Means. DiseÃ±ado para procesar conjuntos de datos masivos aprovechando arquitecturas de **memoria distribuida (OpenMPI)** y **memoria compartida (OpenMP)**.

## ğŸ“Š Rendimiento y AceleraciÃ³n (Speedup)

El objetivo principal de este proyecto es exprimir al mÃ¡ximo el hardware subyacente. Partiendo de una implementaciÃ³n secuencial base, se han aplicado optimizaciones arquitectÃ³nicas severas:

* **Volumen de Datos:** Procesamiento de **8.000.000 de puntos** en **32 dimensiones** (Aprox. 1 GB de RAM en memoria continua).
* **Tiempo Secuencial Base:** ~81.68 segundos.
* **Tiempo HÃ­brido Optimizado:** **~2.74 segundos** (Media de 10 ejecuciones).
* **AceleraciÃ³n Total (Speedup):** **~30x**.

### TÃ©cnicas de OptimizaciÃ³n Aplicadas:
1. **Consciencia de CachÃ© L1/L2:** Uso de vectores contiguos en memoria 1D (`std::vector<float>`) para evitar el *cache-miss* de los punteros dobles y maximizar el pre-fetching.
2. **MetaprogramaciÃ³n (Templates):** Desenrollado de bucles en tiempo de compilaciÃ³n (*Loop Unrolling*) para dimensiones fijas, permitiendo auto-vectorizaciÃ³n SIMD profunda (`-O3 -march=native -ffast-math`).
3. **Paralelismo Bulk Synchronous (MPI):** ReducciÃ³n de la latencia de red agrupando los puntos en "buzones" locales y utilizando comunicaciÃ³n colectiva (`MPI_Alltoallv`, `MPI_Allreduce`) una sola vez por iteraciÃ³n.
4. **Multihilo Seguro (OpenMP):** Uso de buzones privados por hilo (Thread-local storage) y bloques `#pragma omp critical` para evitar *Race Conditions* manteniendo la CPU al 100% de uso.

## ğŸ¥ SimulaciÃ³n de la Arquitectura Distribuida

![AnimaciÃ³n K-Means Distribuido](docs/mpi_kmeans_architecture.gif)

> **Nota visual:** Esta simulaciÃ³n en 2D muestra el comportamiento real de la red. Los **4 colores** representan las memorias RAM fÃ­sicas de los **4 Nodos MPI** distintos. A medida que los centroides se mueven, los puntos viajan por la red cambiando de dueÃ±o (funciÃ³n `MPI_Alltoallv`) hasta alcanzar el equilibrio matemÃ¡tico.

## ğŸ“ Estructura del Proyecto

```text
ğŸ“¦ HPC-Distributed-KMeans
 â”£ ğŸ“‚ src/          # CÃ³digo fuente principal (K-Means distribuido, MPI, EstadÃ­sticas)
 â”£ ğŸ“‚ include/      # Archivos de cabecera (.h)
 â”£ ğŸ“‚ scripts/      # Utilidades y generadores de datos
 â”£ ğŸ“‚ data/         # Carpeta de destino para los datasets binarios masivos
 â”£ ğŸ“œ Makefile      # AutomatizaciÃ³n de la compilaciÃ³n
 â”— ğŸ“œ README.md